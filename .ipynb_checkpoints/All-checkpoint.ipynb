{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file contains all the algorithms together in one file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with our baseline model, KNN.\n",
    "We implemented the KNN on our own without using the one from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import data_process\n",
    "\n",
    "\n",
    "class ModelKNN:\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.training_x = train_x\n",
    "        self.training_t = train_y\n",
    "        self.emotion_data = self.initial_data()\n",
    "\n",
    "    def initial_data(self):\n",
    "        data = {'pixels': [], 'emotion': ''}\n",
    "        t = pd.DataFrame(data=data)\n",
    "        return t\n",
    "\n",
    "    def detect_face(self, img_path):\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "        emotions = ['anger', 'contempt', 'happy', 'sadness']\n",
    "        faceDet = cv2.CascadeClassifier(\"haarcascade\\haarcascade_frontalface_default.xml\")\n",
    "        if img_path == 'video':\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            while True:\n",
    "                k = cv2.waitKey(1)\n",
    "                ret, img = cap.read()\n",
    "                # img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "                # img = cv2.flip(img, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                faces = faceDet.detectMultiScale(\n",
    "                    gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(20, 20)\n",
    "                )\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                    gray = gray[y:y + h, x:x + w]\n",
    "                    # get_landmarks(img)\n",
    "\n",
    "                cv2.imshow('video', img)\n",
    "\n",
    "                if k == 32:\n",
    "                    try:\n",
    "                        gray = cv2.resize(gray, (350, 350))\n",
    "                        print(self.predict(gray, 5))\n",
    "                    except:\n",
    "                        print('error')\n",
    "                if k == 27:\n",
    "                    break\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return 'close'\n",
    "\n",
    "        else:\n",
    "                frame = cv2.imread(img_path)\n",
    "\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5),\n",
    "                                                flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "                if len(face) == 1:\n",
    "                    facefeatures = face\n",
    "                else:\n",
    "                    facefeatures = \"\"\n",
    "\n",
    "                for (x, y, w, h) in facefeatures:\n",
    "                    if facefeatures == \"\":\n",
    "                        print(\"no face found in file: %s\" % img_path)\n",
    "                    else:\n",
    "                        gray = gray[y:y + h, x:x + w]\n",
    "\n",
    "                detections = detector(gray, 1)\n",
    "                for k, d in enumerate(detections):\n",
    "                    shape = predictor(gray, d)\n",
    "                    for i in range(1, 68):\n",
    "                        cv2.circle(gray, (shape.part(i).x, shape.part(i).y), 1, (0, 0, 255), thickness=1)\n",
    "                gray = cv2.resize(gray, (48, 48))\n",
    "                cv2.imwrite(img_path, gray)\n",
    "\n",
    "\n",
    "    def train(self, t_data, label):\n",
    "        for f, b in zip(t_data, label):\n",
    "            try:\n",
    "                self.emotion_data.loc[-1] = [f, b]\n",
    "                self.emotion_data.index = self.emotion_data.index + 1\n",
    "                self.emotion_data = self.emotion_data.sort_index()\n",
    "            except:\n",
    "                print('error in \"' + b)\n",
    "\n",
    "    def euclidean_distance(self, row1, row2):\n",
    "        distance_x = 0.0\n",
    "        distance_x += (row1 - row2)**2\n",
    "        return sum(sum(sum(np.sqrt(distance_x))))\n",
    "\n",
    "\n",
    "    def predict(self, face, k):\n",
    "        distance_d = {'distance': [], 'emotion': ''}\n",
    "        df = pd.DataFrame(data=distance_d)\n",
    "        for i, r in self.emotion_data.iterrows():\n",
    "            distance = self.euclidean_distance(self.emotion_data.iloc[i]['pixels'], face)\n",
    "            df.loc[-1] = [distance, self.emotion_data.iloc[i]['emotion']]\n",
    "            df.index = df.index + 1\n",
    "            df = df.sort_index()\n",
    "        df2 = df.sort_values(by=['distance'], ascending=True, axis=0)[:k]\n",
    "        counter = Counter(df2['emotion'])\n",
    "        prediction = counter.most_common()[0][0]\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    def test(self, p_data, label):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        for f, b in zip(p_data, label):\n",
    "            predict_face = self.predict(f, 5)\n",
    "            if b == predict_face:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        accuracy = (1 / (correct + incorrect)) * correct\n",
    "        print('correct: ' + str(correct) + '\\n' + 'incorrect: ' + str(incorrect) + '\\n' + 'accuracy: ' + str(accuracy) + '\\n' + \"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get training data and test data from dataset 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xKNN,yKNN = data_process.process_data('dataset.csv', \"KNN\")\n",
    "X_trainKNN, X_testKNN, y_trainKNN, y_testKNN = train_test_split(xKNN, yKNN, test_size=0.33, random_state=42)\n",
    "\n",
    "x2KNN,y2KNN = data_process.process_data('dataset2.csv', \"KNN\")\n",
    "X_train2KNN, X_test2KNN, y_train2KNN, y_test2KNN = train_test_split(x2KNN, y2KNN, test_size=0.33, random_state=42)\n",
    "\n",
    "x, y = data_process.process_data('dataset.csv', \"OTHER\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "x2, y2 = data_process.process_data('dataset2.csv', \"OTHER\")\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.33, random_state=42)\n",
    "\n",
    "xCNN, yCNN = data_process.process_data('dataset.csv', \"CNN\")\n",
    "X_trainCNN, X_testCNN, y_trainCNN, y_testCNN = train_test_split(xCNN, yCNN, test_size=0.33, random_state=42)\n",
    "val_data = (X_testCNN,y_testCNN)\n",
    "\n",
    "x2CNN, y2CNN = data_process.process_data('dataset2.csv', \"CNN\")\n",
    "X_train2CNN, X_test2CNN, y_train2CNN, y_test2CNN = train_test_split(x2CNN, y2CNN, test_size=0.33, random_state=42)\n",
    "val_data2 = (X_test2CNN,y_test2CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model and train it with training data, test it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 115\n",
      "incorrect: 44\n",
      "accuracy: 0.7232704402515724\n",
      "--- 97.61059665679932 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_1 = ModelKNN(X_trainKNN, y_trainKNN)\n",
    "model_1.train(X_trainKNN, y_trainKNN)\n",
    "model_1.test(X_testKNN,y_testKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, the testing duration is very long\n",
    "do the same with data from dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again very long testing duration\n",
    "train and test it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 106\n",
      "incorrect: 53\n",
      "accuracy: 0.6666666666666667\n",
      "--- 96.12914395332336 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_2 = ModelKNN(X_train2KNN, y_train2KNN)\n",
    "model_2.train(X_train2KNN, y_train2KNN)\n",
    "model_2.test(X_test2KNN,y_test2KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "metricsKNN = list()\n",
    "cmKNN = dict()\n",
    "\n",
    "y_predKNN = list()\n",
    "y_pred2KNN = list()\n",
    "for a in X_testKNN:\n",
    "    y_predKNN.append(model_1.predict(a,5))\n",
    "\n",
    "for a in X_test2KNN:\n",
    "      y_pred2KNN.append(model_2.predict(a,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_testKNN = enc.fit_transform(y_testKNN)\n",
    "y_predKNN = enc.fit_transform(y_predKNN)\n",
    "\n",
    "y_test2KNN = enc.fit_transform(y_test2KNN)\n",
    "y_pred2KNN = enc.fit_transform(y_pred2KNN)\n",
    "\n",
    "precision, recall, fscore, _ = score(y_testKNN, y_predKNN, average='weighted')\n",
    "accuracy = accuracy_score(y_testKNN, y_predKNN)\n",
    "auc = roc_auc_score(label_binarize(y_testKNN, classes=[0,1,2]),\n",
    "        label_binarize(y_predKNN, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metricsKNN.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'AUC':auc}, name=\"model1\"))\n",
    "cmKNN = confusion_matrix(y_testKNN,y_predKNN)\n",
    "\n",
    "metrics2KNN = list()\n",
    "cm2KNN = dict()\n",
    "\n",
    "precision2, recall2, fscore2, _ = score(y_test2KNN, y_pred2KNN, average='weighted')\n",
    "accuracy2 = accuracy_score(y_test2KNN, y_pred2KNN)\n",
    "auc2 = roc_auc_score(label_binarize(y_test2KNN, classes=[0,1,2]),\n",
    "        label_binarize(y_pred2KNN, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metrics2KNN.append(pd.Series({'precision':precision2, 'recall':recall2, 'fscore':fscore2, 'accuracy':accuracy2,'AUC':auc2}, name=\"model2\"))\n",
    "cm2KNN = confusion_matrix(y_test2KNN,y_pred2KNN)\n",
    "m = metricsKNN + metrics2KNN\n",
    "metricsKNN = pd.concat(m, axis=1)\n",
    "\n",
    "metricsKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "ax = sns.heatmap(cmKNN, annot=True, fmt='d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = sns.heatmap(cm2KNN, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next up we will do the random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "import pydotplus\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "data = tree.export_graphviz(dtree)\n",
    "graph = pydotplus.graph_from_dot_data(data)\n",
    "graph.write_png('mydecisiontree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_predTREE = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img=pltimg.imread('mydecisiontree.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtree2 = DecisionTreeClassifier()\n",
    "dtree2 = dtree2.fit(X_train2, y_train2)\n",
    "data2 = tree.export_graphviz(dtree2)\n",
    "graph2 = pydotplus.graph_from_dot_data(data2)\n",
    "graph2.write_png('mydecisiontree2.png')\n",
    "y_pred2TREE = dtree2.predict(X_test2)\n",
    "img2=pltimg.imread('mydecisiontree2.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_testTREE = enc.fit_transform(y_test)\n",
    "y_predTREE = enc.fit_transform(y_predTREE)\n",
    "\n",
    "y_test2TREE = enc.fit_transform(y_test2)\n",
    "y_pred2TREE = enc.fit_transform(y_pred2TREE)\n",
    "\n",
    "precision, recall, fscore, _ = score(y_testTREE, y_predTREE, average='weighted')\n",
    "accuracy = accuracy_score(y_testTREE, y_predTREE)\n",
    "auc = roc_auc_score(label_binarize(y_testTREE, classes=[0,1,2]),\n",
    "        label_binarize(y_predTREE, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metrics.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'AUC':auc}, name=\"random forest 1\"))\n",
    "cm = confusion_matrix(y_testTREE,y_predTREE)\n",
    "\n",
    "metrics2 = list()\n",
    "cm2 = dict()\n",
    "\n",
    "precision2, recall2, fscore2, _ = score(y_test2TREE, y_pred2TREE, average='weighted')\n",
    "accuracy2 = accuracy_score(y_test2TREE, y_pred2TREE)\n",
    "auc2 = roc_auc_score(label_binarize(y_test2TREE, classes=[0,1,2]),\n",
    "        label_binarize(y_pred2TREE, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metrics2.append(pd.Series({'precision':precision2, 'recall':recall2, 'fscore':fscore2, 'accuracy':accuracy2,'AUC':auc2}, name=\"Random forest 2\"))\n",
    "cm2 = confusion_matrix(y_test2TREE,y_pred2TREE)\n",
    "m = metrics + metrics2\n",
    "metrics = pd.concat(m, axis=1)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = sns.heatmap(cm2, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we will do SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SDGclass = SGDClassifier(loss='log', alpha=0.1, penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SDGclass = SDGclass.fit(X_train, y_train)\n",
    "y_predSGD = SDGclass.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SDGclass2 = SGDClassifier(loss='log', alpha=0.1, penalty='l2')\n",
    "\n",
    "SDGclass2 = SDGclass2.fit(X_train2, y_train2)\n",
    "y_pred2SGD = SDGclass2.predict(X_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_testSGD = enc.fit_transform(y_test)\n",
    "y_predSGD = enc.fit_transform(y_predSGD)\n",
    "\n",
    "y_test2SGD = enc.fit_transform(y_test2)\n",
    "y_pred2SGD = enc.fit_transform(y_pred2SGD)\n",
    "\n",
    "precision, recall, fscore, _ = score(y_testSGD, y_predSGD, average='weighted')\n",
    "accuracy = accuracy_score(y_testSGD, y_predSGD)\n",
    "auc = roc_auc_score(label_binarize(y_testSGD, classes=[0,1,2]),\n",
    "        label_binarize(y_predSGD, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metrics.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'AUC':auc}, name=\"model1\"))\n",
    "cm = confusion_matrix(y_testSGD,y_predSGD)\n",
    "\n",
    "metrics2 = list()\n",
    "cm2 = dict()\n",
    "\n",
    "precision2, recall2, fscore2, _ = score(y_test2SGD, y_pred2SGD, average='weighted')\n",
    "accuracy2 = accuracy_score(y_test2SGD, y_pred2SGD)\n",
    "auc2 = roc_auc_score(label_binarize(y_test2SGD, classes=[0,1,2]),\n",
    "        label_binarize(y_pred2SGD, classes=[0,1,2]),\n",
    "        average='weighted', multi_class='ovr')\n",
    "metrics2.append(pd.Series({'precision':precision2, 'recall':recall2, 'fscore':fscore2, 'accuracy':accuracy2,'AUC':auc2}, name=\"model2\"))\n",
    "cm2 = confusion_matrix(y_test2SGD,y_pred2SGD)\n",
    "m = metrics + metrics2\n",
    "metrics = pd.concat(m, axis=1)\n",
    "\n",
    "metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = sns.heatmap(cm2, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit model and train and compare accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear_pred = linear.predict(X_test)\n",
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "sig_pred = sig.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_lin = linear.score(X_test, y_test)\n",
    "accuracy_poly = poly.score(X_test, y_test)\n",
    "accuracy_rbf = rbf.score(X_test, y_test)\n",
    "accuracy_sig = sig.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy Linear Kernel: ' , accuracy_lin)\n",
    "print('Accuracy Polynomial Kernel: ', accuracy_poly)\n",
    "print('Accuracy Radial Basis Kernel: ', accuracy_rbf)\n",
    "print('Accuracy Sigmoid Kernel: ', accuracy_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same for dataset2.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "linear2 = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train2, y_train2)\n",
    "rbf2 = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train2, y_train2)\n",
    "poly2 = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train2, y_train2)\n",
    "sig2 = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear_pred2 = linear2.predict(X_test2)\n",
    "poly_pred2 = poly2.predict(X_test2)\n",
    "rbf_pred2 = rbf2.predict(X_test2)\n",
    "sig_pred2 = sig2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_lin2 = linear2.score(X_test2, y_test2)\n",
    "accuracy_poly2 = poly2.score(X_test2, y_test2)\n",
    "accuracy_rbf2 = rbf2.score(X_test2, y_test2)\n",
    "accuracy_sig2 = sig2.score(X_test2, y_test2)\n",
    "\n",
    "print('Accuracy Linear Kernel: ' , accuracy_lin2)\n",
    "print('Accuracy Polynomial Kernel: ', accuracy_poly2)\n",
    "print('Accuracy Radial Basis Kernel: ', accuracy_rbf2)\n",
    "print('Accuracy Sigmoid Kernel: ', accuracy_sig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#\n",
    "# y_pred = list()\n",
    "# y_pred2 = list()\n",
    "# for a in X_test:\n",
    "#     y_pred.append(linear.predict(a))\n",
    "#\n",
    "# for a in X_test2:\n",
    "#       y_pred2.append(linear2.predict(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "\n",
    "labels = ['Linear', 'Polynomial', 'Radial Basis', 'Sigmoid']\n",
    "models = [linear, poly, rbf, sig]\n",
    "\n",
    "\n",
    "y_predSVM = list()\n",
    "for label, model in zip(labels, models):\n",
    "    y_predSVM.append(pd.Series(enc.fit_transform(model.predict(X_test)), name=label))\n",
    "y_predSVM = pd.concat(y_predSVM, axis=1)\n",
    "y_testSVM = enc.fit_transform(y_test)\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    precision, recall, fscore, _ = score(y_testSVM, y_predSVM[label], average='weighted')\n",
    "\n",
    "    accuracy = accuracy_score(y_testSVM, y_predSVM[label])\n",
    "\n",
    "    auc = roc_auc_score(label_binarize(y_testSVM, classes=[0,1,2,3]),\n",
    "              label_binarize(y_predSVM[label], classes=[0,1,2,3]),\n",
    "              average='weighted', multi_class='ovr')\n",
    "\n",
    "    cm[label] = confusion_matrix(y_testSVM, y_predSVM[label])\n",
    "\n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'auc':auc }, name=label))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels2 = ['Linear', 'Polynomial', 'Radial Basis', 'Sigmoid']\n",
    "models2 = [linear2, poly2, rbf2, sig2]\n",
    "\n",
    "y_pred2SVM = list()\n",
    "\n",
    "y_test2SVM = enc.fit_transform(y_test2)\n",
    "\n",
    "for label, model in zip(labels2, models2):\n",
    "    y_pred2SVM.append(pd.Series(enc.fit_transform(model.predict(X_test2)), name=label))\n",
    "y_pred2SVM = pd.concat(y_pred2SVM, axis=1)\n",
    "\n",
    "metrics2 = list()\n",
    "cm2 = dict()\n",
    "\n",
    "for label in labels2:\n",
    "\n",
    "    precision, recall, fscore, _ = score(y_test2SVM, y_pred2SVM[label], average='weighted')\n",
    "\n",
    "    accuracy = accuracy_score(y_test2SVM, y_pred2SVM[label])\n",
    "\n",
    "    auc = roc_auc_score(label_binarize(y_test2SVM, classes=[0,1,2]),\n",
    "              label_binarize(y_pred2SVM[label], classes=[0,1,2]),\n",
    "              average='weighted', multi_class='ovr')\n",
    "\n",
    "    cm2[label] = confusion_matrix(y_test2SVM, y_pred2SVM[label])\n",
    "\n",
    "    metrics2.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'auc':auc }, name=label))\n",
    "\n",
    "metrics2 = pd.concat(metrics2, axis=1)\n",
    "\n",
    "metrics2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fig, axList = plt.subplots(nrows=1, ncols=4)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(18, 6)\n",
    "\n",
    "for ax,label in zip(axList, labels):\n",
    "    sns.heatmap(cm[label], ax=ax, annot=True, fmt='d')\n",
    "    ax.set(title=label)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=1, ncols=4)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(18, 6)\n",
    "\n",
    "for ax,label in zip(axList, labels):\n",
    "    sns.heatmap(cm2[label], ax=ax, annot=True, fmt='d')\n",
    "    ax.set(title=label)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do a deep learning algorithm, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the modules and get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import cv2\n",
    "import data\n",
    "from PIL import Image\n",
    "import dlib\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import keras for the model training and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the model with it's layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape=(48, 48, 1)\n",
    "num_classes = 4\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same', input_shape=input_shape))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data generator Generate batches of tensor image data with real-time data augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "batch_size = 32\n",
    "opt = RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist_model_1 = model_1.fit_generator(data_generator.flow(X_trainCNN, y_trainCNN,\n",
    "                                            batch_size),\n",
    "                        epochs=20, verbose=1,validation_data =val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "score = model_1.evaluate(X_testCNN, y_testCNN, verbose=1)\n",
    "history_dict=hist_model_1.history\n",
    "history_dict.keys()\n",
    "\n",
    "train_loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same with dataset2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape=(48, 48, 1)\n",
    "num_classes = 4\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_2.add(Conv2D(32, (5, 5), strides = (2,2), padding='same', input_shape=input_shape))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_2.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_generator2 = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "batch_size = 32\n",
    "opt2 = RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist_model_2 = model_2.fit_generator(data_generator2.flow(X_train2CNN, y_train2CNN,\n",
    "                                            batch_size),\n",
    "                        epochs=20, verbose=1,validation_data =val_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score2 = model_2.evaluate(X_test2CNN, y_test2CNN, verbose=1)\n",
    "history_dict2=hist_model_2.history\n",
    "history_dict2.keys()\n",
    "\n",
    "train_loss_values2 = history_dict2['loss']\n",
    "val_loss_values2 = history_dict2['val_loss']\n",
    "\n",
    "epochs2 = range(1, len(history_dict2['accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs2, train_loss_values2, 'bo', label='Training loss')\n",
    "plt.plot(epochs2, val_loss_values2, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare classification metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "y_predCNN = model_1.predict(X_testCNN)\n",
    "y_pred_class = model_1.predict_classes(X_testCNN, verbose=0)\n",
    "rounded_labels = np.argmax(y_testCNN, axis=1)\n",
    "\n",
    "precision, recall, fscore, _ = score(rounded_labels, y_pred_class, average='weighted')\n",
    "accuracy = accuracy_score(rounded_labels, y_pred_class)\n",
    "auc = roc_auc_score(y_testCNN, y_predCNN)\n",
    "metrics.append(pd.Series({'precision':precision, 'recall':recall, 'fscore':fscore, 'accuracy':accuracy, 'auc':auc}, name=\"model1\"))\n",
    "cm = confusion_matrix(rounded_labels,y_pred_class)\n",
    "\n",
    "metrics2 = list()\n",
    "cm2 = dict()\n",
    "y_pred2CNN = model_2.predict(X_test2CNN)\n",
    "y_pred_class2 = model_2.predict_classes(X_test2CNN, verbose=0)\n",
    "rounded_labels2 = np.argmax(y_test2CNN, axis=1)\n",
    "\n",
    "precision2, recall2, fscore2, _ = score(rounded_labels2, y_pred_class2, average='weighted')\n",
    "accuracy2 = accuracy_score(rounded_labels2, y_pred_class2)\n",
    "auc2 = roc_auc_score(y_test2CNN, y_pred2CNN)\n",
    "metrics2.append(pd.Series({'precision':precision2, 'recall':recall2, 'fscore':fscore2, 'accuracy':accuracy2, 'auc':auc2}, name=\"model2\"))\n",
    "cm2 = confusion_matrix(rounded_labels2,y_pred_class2)\n",
    "m = metrics + metrics2\n",
    "metrics = pd.concat(m, axis=1)\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = sns.heatmap(cm2, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
